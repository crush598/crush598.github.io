import{_ as t}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as r,c as s,a as i,b as e,e as a,f as l,r as h}from"./app.1526ec34.js";const o={},d=l('<h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介" aria-hidden="true">#</a> 简介</h2><ul><li><p>AI、ML、深度学习和数据科学之间关系的图表</p></li><li><p><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208191645653.png" alt="AI、ML、深度学习和数据科学之间关系"></p></li><li><p>监督学习</p></li><li><p>无监督学习</p></li><li><p>特征选取--》过拟合</p></li></ul><h2 id="机器学习的历史" tabindex="-1"><a class="header-anchor" href="#机器学习的历史" aria-hidden="true">#</a> 机器学习的历史</h2><h2 id="机器学习的流程" tabindex="-1"><a class="header-anchor" href="#机器学习的流程" aria-hidden="true">#</a> 机器学习的流程</h2><ol><li>收集整理数据----<strong>特征选择和特征提取</strong></li><li>可视化数据、拆分数据集（训练集、测试集）</li><li><strong>建立模型----训练模型、评估模型</strong></li><li>参数调优<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202209192123267.png" alt="iShot_2022-09-19_21.01.29" style="zoom:50%;"></li></ol><h2 id="线性回归" tabindex="-1"><a class="header-anchor" href="#线性回归" aria-hidden="true">#</a> 线性回归</h2><ul><li><p>模型和损失函数</p><ul><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208211500278.png" alt="线性回归的简述"></li></ul></li></ul><h3 id="梯度下降" tabindex="-1"><a class="header-anchor" href="#梯度下降" aria-hidden="true">#</a> 梯度下降</h3>',8),c=i("li",null,[i("img",{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208222056094.png",alt:"一元与多元线性回归的梯度下降",style:{zoom:"50%"}})],-1),g={id:"梯度下降法的矩阵方式描述-代码实现比较简洁",tabindex:"-1"},u=i("a",{class:"header-anchor",href:"#梯度下降法的矩阵方式描述-代码实现比较简洁","aria-hidden":"true"},"#",-1),m={href:"https://wangcongying.com/2019/09/26/summaryOfGradientDescent/#toc-heading-43",target:"_blank",rel:"noopener noreferrer"},p=l('<ul><li><h3 id="批量梯度下降法-batch-gradient-descent-bgd" tabindex="-1"><a class="header-anchor" href="#批量梯度下降法-batch-gradient-descent-bgd" aria-hidden="true">#</a> 批量梯度下降法（Batch Gradient Descent, BGD）</h3></li><li><h3 id="随机梯度下降法-stochastic-gradient-descent-sgd" tabindex="-1"><a class="header-anchor" href="#随机梯度下降法-stochastic-gradient-descent-sgd" aria-hidden="true">#</a> 随机梯度下降法（Stochastic Gradient Descent, SGD）</h3></li><li><h3 id="小批量梯度下降法-mini-batch-gradient-descent-mbgd" tabindex="-1"><a class="header-anchor" href="#小批量梯度下降法-mini-batch-gradient-descent-mbgd" aria-hidden="true">#</a> 小批量梯度下降法（Mini-batch Gradient Descent, MBGD）</h3></li></ul>',1),_=i("li",null,[i("p",null,[i("strong",null,"多元线性回归")])],-1),b=l('<blockquote><ul><li>向量的点积</li><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208231905636.png" alt="向量的点积" style="zoom:40%;"></li></ul></blockquote><h3 id="高阶优化方法" tabindex="-1"><a class="header-anchor" href="#高阶优化方法" aria-hidden="true">#</a> 高阶优化方法</h3><ul><li>Adam Algorithm</li></ul><h2 id="分类" tabindex="-1"><a class="header-anchor" href="#分类" aria-hidden="true">#</a> 分类</h2><h3 id="logistic-回归" tabindex="-1"><a class="header-anchor" href="#logistic-回归" aria-hidden="true">#</a> Logistic 回归</h3><ul><li><p>Sigmoid function</p></li><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208252119737.png" alt="Sigmoid function" style="zoom:50%;"></li><li><p>决策边界 decision boundary</p></li><li><p><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208252337873.jpg" alt="decision boundary"></p></li><li><p>损失函数 Loss Function</p></li><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208252334800.png" alt="Loss Function" style="zoom:50%;"></li><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208252343175.jpg" alt="损失函数" style="zoom:50%;"></li><li><p>梯度下降</p></li><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208281349036.png" alt="梯度下降" style="zoom:50%;"><p>尽管梯度下降的方法在线性回归和 Logistic 回归上的表现形式一样；当是两者的函数的定义是不同的。</p></li></ul><h2 id="神经网络-neural-networks" tabindex="-1"><a class="header-anchor" href="#神经网络-neural-networks" aria-hidden="true">#</a> 神经网络 Neural networks</h2><h3 id="感知机-前身" tabindex="-1"><a class="header-anchor" href="#感知机-前身" aria-hidden="true">#</a> 感知机（前身）</h3><h3 id="神经网络的简介" tabindex="-1"><a class="header-anchor" href="#神经网络的简介" aria-hidden="true">#</a> 神经网络的简介</h3><ul><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208292210256.png" alt="iShot_2022-08-29_22.08.54"></li><li><strong>正向传播</strong></li><li>激活函数</li><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202209130940988.png" alt="激活函数" style="zoom:50%;"></li><li><strong>Softmax 函数</strong></li><li>隐藏层</li><li><strong>卷积层----仅计算部分输入</strong></li><li>输出层</li><li><strong>反向传播</strong></li></ul><h3 id="如何提升算法的性能" tabindex="-1"><a class="header-anchor" href="#如何提升算法的性能" aria-hidden="true">#</a> 如何提升算法的性能</h3><ul><li>划分训练集和测试集</li><li>交叉验证</li><li>建立表现基准</li><li>绘制学习曲线</li></ul><h2 id="决策树" tabindex="-1"><a class="header-anchor" href="#决策树" aria-hidden="true">#</a> 决策树</h2><ul><li>决定在节点使用什么特征进行划分</li><li>停止划分的条件</li><li>剪枝？？？</li><li><strong><u>回归树</u></strong></li><li>集成学习--随机森林--bagging--bosting</li></ul><h2 id="聚类算法-k-means" tabindex="-1"><a class="header-anchor" href="#聚类算法-k-means" aria-hidden="true">#</a> 聚类算法 K--means</h2><ul><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202209272040606.png" alt="K - means 算法流程 " style="zoom:50%;"></li></ul><h2 id="tips" tabindex="-1"><a class="header-anchor" href="#tips" aria-hidden="true">#</a> <strong>tips</strong></h2>',17),f=i("p",null,"数据预处理",-1),x={href:"https://www.cnblogs.com/HuZihu/p/9761161.html",target:"_blank",rel:"noopener noreferrer"},v=l("<ul><li><p><strong>最大最小值归一化（min-max normalization）</strong>：将数值范围缩放到 [0, 1] 区间里</p></li><li><p><strong>均值归一化（mean normalization）</strong>：将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为0</p></li><li><p><strong>标准化 / z值归一化（*<em>s**tandardization /**</em>* z-score *<em>normalization*</em>*<em>）*</em></strong>：将数值缩放到0附近，且数据的分布变为均值为0，标准差为1的标准正态分布（先减去均值来对特征进行 <strong>中心化 mean centering</strong> 处理，再除以标准差进行缩放）</p></li></ul>",1),k=i("ul",null,[i("li",null,"通过绘制学习曲线来帮助判断梯度下降是否收敛--学习率的选择")],-1),z={href:"https://www.cnblogs.com/peizhe123/p/7412364.html",target:"_blank",rel:"noopener noreferrer"},j=l('<li><p>过拟合 Overfitting</p></li><li><img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202209130927330.png" alt="iShot_2022-09-13_09.27.11" style="zoom:50%;"><blockquote><ul><li>Bias -- 模型在训练集上的误差</li><li>Variance -- 训练集误差和测试集误差之间的差距</li><li>解决方法--过拟合<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208281416273.png" alt="解决方法--过拟合" style="zoom:50%;"><ol><li>收集更多的训练数据</li><li>减少使用的特征（feature）的数量--即挑选更有用的特征</li><li><strong><u>正则化</u></strong></li></ol></li></ul></blockquote></li>',2),y={href:"https://lavi-liu.blog.csdn.net/article/details/99984288",target:"_blank",rel:"noopener noreferrer"},w=i("ul",null,[i("li",null,"参考内容：https://lulaoshi.info/machine-learning/linear-model/regularization")],-1);function S(L,B){const n=h("ExternalLinkIcon");return r(),s("div",null,[d,i("ul",null,[c,i("li",null,[i("h4",g,[u,e(),i("a",m,[e("梯度下降法的矩阵方式描述"),a(n)]),e("(代码实现比较简洁)")]),p]),_]),b,i("ul",null,[i("li",null,[f,i("blockquote",null,[i("p",null,[i("a",x,[e("特征缩放"),a(n)])]),v]),k,i("blockquote",null,[i("p",null,[i("a",z,[e("特征工程"),a(n)])])])]),j,i("li",null,[i("p",null,[i("a",y,[e("正则化"),a(n)])]),w])])])}const q=t(o,[["render",S],["__file","01. 机器学习初学者.html.vue"]]);export{q as default};
