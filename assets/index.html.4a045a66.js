const e=JSON.parse('{"key":"v-53d6fc0b","path":"/posts/02.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E5%AD%A6%E8%80%85/","title":"机器学习初学者","lang":"zh-CN","frontmatter":{"title":"机器学习初学者","date":"2022-08-21T23:53:25.000Z","category":["AI"],"tag":["机器学习"],"summary":"简介 AI、ML、深度学习和数据科学之间关系的图表; ; 监督学习; 无监督学习; 特征选取--》过拟合; 机器学习的历史 机器学习的流程 1. 收集整理数据----特征选择和特征提取 2. 可视化数据、拆分数据集（训练集、测试集） 3. 建立模型----训练模型、评估模型 4. 参数调优 线性回归 模型和损失函数; ; 梯度下降 ; #### 梯度下降法的","head":[["meta",{"property":"og:url","content":"https://crush598.github.io/posts/02.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E5%AD%A6%E8%80%85/"}],["meta",{"property":"og:site_name","content":"Hush"}],["meta",{"property":"og:title","content":"机器学习初学者"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:locale:alternate","content":"en-US"}],["meta",{"property":"article:tag","content":"机器学习"}],["meta",{"property":"article:published_time","content":"2022-08-21T23:53:25.000Z"}],["link",{"rel":"alternate","hreflang":"en-us","href":"https://crush598.github.io/en/posts/02.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E5%AD%A6%E8%80%85/"}]]},"excerpt":"","headers":[{"level":2,"title":"简介","slug":"简介","link":"#简介","children":[]},{"level":2,"title":"机器学习的历史","slug":"机器学习的历史","link":"#机器学习的历史","children":[]},{"level":2,"title":"机器学习的流程","slug":"机器学习的流程","link":"#机器学习的流程","children":[]},{"level":2,"title":"线性回归","slug":"线性回归","link":"#线性回归","children":[{"level":3,"title":"梯度下降","slug":"梯度下降","link":"#梯度下降","children":[]},{"level":3,"title":"批量梯度下降法（Batch Gradient Descent, BGD）","slug":"批量梯度下降法-batch-gradient-descent-bgd","link":"#批量梯度下降法-batch-gradient-descent-bgd","children":[]},{"level":3,"title":"随机梯度下降法（Stochastic Gradient Descent, SGD）","slug":"随机梯度下降法-stochastic-gradient-descent-sgd","link":"#随机梯度下降法-stochastic-gradient-descent-sgd","children":[]},{"level":3,"title":"小批量梯度下降法（Mini-batch Gradient Descent, MBGD）","slug":"小批量梯度下降法-mini-batch-gradient-descent-mbgd","link":"#小批量梯度下降法-mini-batch-gradient-descent-mbgd","children":[]},{"level":3,"title":"高阶优化方法","slug":"高阶优化方法","link":"#高阶优化方法","children":[]}]},{"level":2,"title":"分类","slug":"分类","link":"#分类","children":[{"level":3,"title":"Logistic 回归","slug":"logistic-回归","link":"#logistic-回归","children":[]}]},{"level":2,"title":"神经网络 Neural networks","slug":"神经网络-neural-networks","link":"#神经网络-neural-networks","children":[{"level":3,"title":"感知机（前身）","slug":"感知机-前身","link":"#感知机-前身","children":[]},{"level":3,"title":"神经网络的简介","slug":"神经网络的简介","link":"#神经网络的简介","children":[]},{"level":3,"title":"如何提升算法的性能","slug":"如何提升算法的性能","link":"#如何提升算法的性能","children":[]}]},{"level":2,"title":"决策树","slug":"决策树","link":"#决策树","children":[]},{"level":2,"title":"聚类算法  K--means","slug":"聚类算法-k-means","link":"#聚类算法-k-means","children":[]},{"level":2,"title":"tips","slug":"tips","link":"#tips","children":[]}],"git":{},"readingTime":{"minutes":3.44,"words":1031},"copyright":"著作权归Hush所有\\n原文链接：https://crush598.github.io/posts/02.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E5%AD%A6%E8%80%85/","filePathRelative":"posts/02. 机器学习初学者/README.md","localizedDate":"2022年8月22日"}');export{e as data};
